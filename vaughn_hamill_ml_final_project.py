# -*- coding: utf-8 -*-
"""Vaughn_Hamill_ML_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZspvqUkWG0ByUqKC2FTM4ERhbvkAdJ97

# Imports, Functions, and Variables
"""

# Import all packages
import math
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.pyplot import imshow
import sklearn as sklearn
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# If importing data from Google Drive:
# Connect Google Drive
from google.colab import drive

try:
  drive.mount("/content/drive", force_remount=True)
  drive_loc = "/content/drive/MyDrive/Colab Notebooks/Machine Learning in Geology/ML Project"

  df = pd.read_excel(f"{drive_loc}/ML_Project.xlsx")
  print("XLSX found")
  df.to_csv(f"{drive_loc}/ML_Project.csv", index=False)
  print("XLSX converted to CSV")
except FileNotFoundError:
  print("Dataset cannot be found")

# If importing data from Google Colab
try:
  file_loc = "/content/sample_data"

  df = pd.read_excel(f"{drive_loc}/ML_Project.xlsx")
  print("XLSX found")
  df.to_csv(f"{drive_loc}/ML_Project.csv", index=False)
  print("XLSX converted to CSV")
except FileNotFoundError:
  print("Dataset cannot be found")

# Calling all functions
# Biplot function with k-mean labels
def biplot_clusters(PCs,coef,klabels,labels=None):
    plt.figure(figsize=(10,10))
    xs = PCs[:,0] # PC1
    ys = PCs[:,1] # PC2
    coef = np.transpose(coef)
    n = coef.shape[0]
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())

    plt.scatter(xs * scalex,ys * scaley, c = klabels)

    for i in range(n):
        plt.arrow(0, 0, coef[i,0],
                  coef[i,1],color = 'purple',
                  alpha = 0.5)
        plt.text(coef[i,0]* 1.15,
                 coef[i,1] * 1.15,
                 labels[i],
                 color = 'darkblue',
                 ha = 'center',
                 va = 'center')

    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title('Biplot')

# PCA function
pca = PCA()

# Variable dictionary
var_dict = {
    "Environment": ["Lake", "Fjord", "Man-Made", "Hot-Spring System", "Mountain", "Island"],
    "Sample_type": ["Lakebed", "Active hydrothermal vent", "Inactive hydrothermal vent", "Relict hydrothermal vent", "Gossan", "Precipitate", "Ooid sand", "Shale"],
    "Apparent_dominant_ion": ["Sulfur", "Iron", "Magnesium", "Unknown"],
    "TOC_supports_life_development": ["No", "Yes"],
    "Biomass_supports_life": ["Low to no microbial input", "Suggests microbial input", "Strong evidence of recent or active microbial input"],
    "Bacteria_presence": ["Low to no microbial input", "Suggests microbial input", "Strong evidence of recent or active microbial input"]
}

"""# Data Review"""

df.shape

print(df.isna().values.any())

df.nunique()

df.describe()

df.info()

df = df.drop(columns=["Sample_name"]) # Remove text from df before analysis
df.head()

# Run a general correlation matrix to understand if any variables are influencing each other
corr = df.corr()
corr.style.background_gradient(cmap="coolwarm", axis=None, vmin=0.50) # shows colormap of values higher than 0.50. Can change the cutoff value with 'vmin'

"""# Geological Setting Variable Grouping"""

# Remove samples that have NaN
clean_df = df.dropna(subset=['Biomass_supports_life']) # If there is no data in biomass, then no data is available for bacteria either

# Geological settings as the culmination of environment and sample type. Apparent dominant ion and/or mineralogy data will be added once available
geo_set_df = clean_df[["Environment", "Sample_type"]].copy()
# Normalize geological settings data
norm_geo_set_df = (geo_set_df - (geo_set_df.mean())) / (geo_set_df.std())

# Apply PCA to geological settings data
geo_set_pca = PCA(n_components=1)
geo_set_pca.fit(norm_geo_set_df)
geo_set_df["geo_set"] = geo_set_pca.fit_transform(norm_geo_set_df)
geo_set_df

# Create xaxis tick marks for later use on charts based on PC
geo_set_values = sorted(geo_set_df['geo_set'].unique())
geo_set_values_r = []

for i in geo_set_values:
  num = round(i, 2)
  geo_set_values_r.append(num)

"""# Geological Setting vs. TOC, Biomass, and Bacteria"""

# Visualize geo_set PC to TOC %
for key, value in list(var_dict.items())[3:]:
  plt.figure(figsize=(6,3), dpi=200)
  plt.title(f"geo_set vs TOC % [Color code: {key}]")

  ax1 = plt.gca()

  ax1.set_ylabel("TOC %")
  ax1.set_xlabel("geo_set PC")

  sct = plt.scatter(geo_set_df['geo_set'], clean_df['TOC'],
                    c=clean_df[f"{key}"],
                    s=1)

  ax1.set_xticks(geo_set_values_r)
  ax1.legend(handles=sct.legend_elements()[0],
             labels=var_dict.get(f"{key}"))
  plt.tight_layout()
  plt.show()

# Drop two high TOC samples from df (index 35 and 45 in full df)
new_clean_df = clean_df[clean_df['TOC'] <= 6]

# Normalize data
new_geo_set_df = new_clean_df[["Environment", "Sample_type"]].copy()
new_norm_geo_set_df = (new_geo_set_df - (new_geo_set_df.mean())) / (new_geo_set_df.std())

# Apply PCA
new_geo_set_pca = PCA(n_components=1)
new_geo_set_pca.fit(new_norm_geo_set_df)
new_geo_set_df["geo_set"] = geo_set_pca.fit_transform(new_norm_geo_set_df)
print(new_geo_set_df.shape)

# Create new xaxis tick marks for charts
geo_set_values = sorted(new_geo_set_df['geo_set'].unique())
geo_set_values_r = []

for i in geo_set_values:
  num = round(i, 2)
  geo_set_values_r.append(num)

new_geo_set_df

# Visualize geo_set PC to TOC % without high TOC % samples
for key, value in list(var_dict.items())[3:]:
  plt.figure(figsize=(6,3), dpi=200)
  plt.title(f"geo_set vs TOC % [Color code: {key}]")

  ax1 = plt.gca()

  ax1.set_ylabel("TOC %")
  ax1.set_xlabel("geo_set PC")

  sct = plt.scatter(new_geo_set_df['geo_set'], new_clean_df['TOC'],
                    c=new_clean_df[f"{key}"],
                    s=1)
  ax1.set_xticks(geo_set_values_r)
  ax1.legend(handles=sct.legend_elements()[0],
             labels=var_dict.get(f"{key}"))
  plt.tight_layout()
  plt.show()

# Visualize geo_set PC to biomass_supports_life
for key, value in list(var_dict.items())[3:]:
  plt.figure(figsize=(6,3), dpi=200)
  plt.title(f"geo_set vs Biomass_supports_life [Color code: {key}]")

  ax1 = plt.gca()

  ax1.set_ylabel("Biomass_supports_life")
  ax1.set_xlabel("geo_set PC")

  sct = plt.scatter(geo_set_df['geo_set'], clean_df['Biomass_supports_life'],
                    c=clean_df[f"{key}"],
                    s=1)
  ax1.set_xticks(geo_set_values_r)
  ax1.legend(handles=sct.legend_elements()[0],
             labels=var_dict.get(f"{key}"))
  plt.tight_layout()
  plt.show()

# Visualize geo_set PC to bacteria_presence
for key, value in list(var_dict.items())[3:]:
  plt.figure(figsize=(6,3), dpi=200)
  plt.title(f"geo_set vs Bacteria_presence [Color code: {key}]")

  ax1 = plt.gca()

  ax1.set_ylabel("Bacteria_presence")
  ax1.set_xlabel("geo_set PC")

  sct = plt.scatter(geo_set_df['geo_set'], clean_df['Bacteria_presence'],
                    c=clean_df[f"{key}"],
                    s=1)
  ax1.set_xticks(geo_set_values_r)
  ax1.legend(handles=sct.legend_elements()[0],
             labels=var_dict.get(f"{key}"))
  plt.tight_layout()
  plt.show()

"""# TOC Supports Life Development Data Prep"""

# Prep new copy of df for TOC logistic regression analysis
toc_df = df[["Environment", "Sample_type", "TOC_supports_life_development", "Biomass_supports_life", "Bacteria_presence"]].copy()
clean_toc_df = toc_df.dropna()
clean_toc_df.shape

"""# TOC Supports Life Development Logistic Regression"""

# Drop TOC_supports_life_development column, apply PCA, and determine PCs
clean_toc_df_copy = clean_toc_df.copy()
clean_toc_df_copy.drop(columns=["TOC_supports_life_development"], inplace=True)

norm_clean_toc_df = (clean_toc_df_copy - clean_toc_df_copy.mean()) / clean_toc_df_copy.std()

pca.fit(norm_clean_toc_df)
toc_pcs = pca.fit_transform(norm_clean_toc_df)

# PCs required to capture 90% variance
toc_pcs_ex = pca.explained_variance_ratio_
print(f"# of PCs: {len(toc_pcs_ex)}")

toc_pc90 = 0
toc_pc_cnt = 1

for pc in toc_pcs_ex:
  pc += toc_pc90
  if pc > 0.90:
    toc_pc90 = pc
    print(f"PC# {toc_pc_cnt}: {pc}")
    break
  else:
    toc_pc90 = pc
    print(f"PC# {toc_pc_cnt}: {pc}")
    toc_pc_cnt += 1

print(f"\nNumber of PCs to capture 90% of variance: \n PCs: {toc_pc_cnt} \n PC % with {toc_pc_cnt}: {round(toc_pc90*100, 2)}%")

# Number of TOC_supports_life_development data points in df
toc_no_yes = clean_toc_df['TOC_supports_life_development'].value_counts()
print(f"Supports life: {toc_no_yes[1]} \nDoes not support life: {toc_no_yes[0]}")

# Prepare training and test sets for logistic regression
target = clean_toc_df['TOC_supports_life_development']
features = toc_pcs # Use of all PCs due to small dataset

# Scale features to help minimize small dataset bias
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Run GridSearchCV test to determine best 'C' for logistic regression training
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(LogisticRegression(penalty='l2', class_weight='balanced'),
                    param_grid, cv=5, scoring='accuracy')
grid.fit(features_scaled, target)

print(f"Best C: {grid.best_params_['C']} with score: {grid.best_score_}")

# Split training and test sets for logistic regression
x_train, x_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2)

train_pct = 100*len(x_train)/len(features_scaled)
test_pct = 100*x_test.shape[0]/features_scaled.shape[0]
print(f'Training data is {train_pct:.3}% of the total data. \nTest data makes up the remaining {test_pct:0.3}%.')

# Logistic regression on training data with L2 regularization
log_reg = LogisticRegression(class_weight="balanced", penalty='l2', C=0.01) # Use of balanced class weight to minimize imbalance bias. 'C' determined from GridSearchCV function
log_reg.fit(x_train, y_train)

print(f"Intercept: {log_reg.intercept_} \nCoefficients: {log_reg.coef_} \nTraining score: {log_reg.score(x_train, y_train)*100}%")

# Cross-validate logistic regression training score to determine if it is providing an accurate model
scores = cross_val_score(log_reg, features_scaled, target, cv=5, scoring='accuracy')
print(f"CV Accuracy: {scores.mean()*100:.2f}% Â± {scores.std()*100:.2f}")

# Apply logistic regression to test data
y_test_pred = log_reg.predict(x_test)

# Adjust threshold for logistic regression to help adjust class imbalance
threshold = 0.3  # Lower threshold = more positives
y_pred_custom = (y_test_pred >= threshold).astype(int)

# Compute accuracy for test and training data
acc_test = (metrics.accuracy_score(y_test, y_pred_custom))*100
acc_train = (log_reg.score(x_train, y_train))*100
print(f"Test accuracy: {acc_test:.4}% \nTraining accuracy: {acc_train:.3}%")

# Create a confusion matrix for test set to visually see logistic regression test results
conf_mat = metrics.confusion_matrix(y_test, y_pred_custom)
metrics.ConfusionMatrixDisplay(conf_mat, display_labels = [False, True]).plot()

# Apply logistic regression to full dataset
full_toc_pred = log_reg.predict_proba(features)
print(full_toc_pred.shape)

# Add logistic regression predictions to df
clean_toc_df_copy["TOC Life Development Prediction"] = full_toc_pred[:,1]
clean_toc_df_copy

# Compute and plot correlation matrix between original data variables and output probabilities
corr_matrix = clean_toc_df_copy.corr()
corr_matrix.style.background_gradient(axis=None)

"""# K-Means Clustering"""

# Create new df copy for KMeans analysis, apply PCA, and determine PCs
kmeans_df_copy = clean_toc_df.copy()

norm_kmeans_df = (kmeans_df_copy - kmeans_df_copy.mean()) / kmeans_df_copy.std()

pca.fit(norm_kmeans_df)
kmeans_pcs = pca.fit_transform(norm_kmeans_df)

# PCs required to capture 90% variance
kmeans_pcs_ex = pca.explained_variance_ratio_
print(f"# of PCs: {len(kmeans_pcs_ex)}")

kmeans_pc90 = 0
kmeans_pc_cnt = 1

for pc in kmeans_pcs_ex:
  pc += kmeans_pc90
  if pc > 0.90:
    kmeans_pc90 = pc
    print(f"PC# {kmeans_pc_cnt}: {pc}")
    break
  else:
    kmeans_pc90 = pc
    print(f"PC# {kmeans_pc_cnt}: {pc}")
    kmeans_pc_cnt += 1

print(f"\nNumber of PCs to capture 90% of variance: \n PCs: {kmeans_pc_cnt} \n PC % with {kmeans_pc_cnt}: {round(kmeans_pc90*100, 2)}%")

# Optional: Add PC values to df
for i in range(0,len(kmeans_pcs_ex)):
  K_pc = kmeans_pcs[:,i]
  i = i+1
  kmeans_df_copy[f"kmeans_pc{i}"] = K_pc

kmeans_df_copy.head()

# Apply KMean clustering to PCs (>90%) with silhouette scores
print(f"Silhouette scores based on cluster size:")
kmeans_pcs_90 = kmeans_pcs # Use of all PCs due to small dataset

# Determine best silhouette score to use for KMean analysis
scores = []

for i in range(2,15):
  kmeans_model = KMeans(n_clusters=i)
  kmeans_model.fit(kmeans_pcs_90)
  label = kmeans_model.predict(kmeans_pcs_90)
  silh = silhouette_score(kmeans_pcs_90, label)
  print(f"{i}-clusters: {silh}")
  scores.append(silh)

# Plot silhouette scores based on cluster number
plt.plot(range(2, 15), scores, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Scores vs. n_clusters")
plt.show()

# Run elbow method to determine optimal number of clusters for KMean analysis
inertias = []
for i in range(2, 15):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(kmeans_pcs_90)
    inertias.append(kmeans.inertia_)

# Plot elbow method data
plt.plot(range(2, 15), inertias, marker='o')
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

# Run KMean analysis based off of silhouette score and elbow method data
nc = 3
kmeans = KMeans(n_clusters=nc)
kmeans.fit(kmeans_pcs_90)
label = kmeans.predict(kmeans_pcs_90)
silh = silhouette_score(kmeans_pcs_90, label)

# Determine cluster labels and add to df
kmeans_df_copy["Cluster_Label"] = kmeans.labels_
kmeans_df_copy.head()

# Create scatter plot to visualize KMeans data
plt.figure(figsize=(10, 7))
plt.scatter(kmeans_pcs_90[:, 0], kmeans_pcs_90[:, 1], c=label, cmap='viridis')
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title(f"K-Means Clustering of Life Development Data (n_clusters={nc})")
plt.colorbar(label="Cluster Label", ticks=([0,1,2]))
plt.show()

# Count how many samples in each cluster
print(kmeans_df_copy['Cluster_Label'].value_counts())

# Compare distributions of key features by cluster
cluster_summary = kmeans_df_copy.groupby('Cluster_Label')[[
    'Environment', 'Sample_type', 'TOC_supports_life_development',
    'Biomass_supports_life', 'Bacteria_presence'
]].mean()

cluster_summary

# Create heatmap to visualize cluster_summmary data
t_cluster_summary = cluster_summary.T
sns.heatmap(t_cluster_summary, annot=True, cmap="viridis")
plt.title("Feature Proportions by Cluster")
plt.show()

# Call biplot function based on KMeans data
biplot_clusters(kmeans_pcs, pca.components_, label, list(kmeans_df_copy.columns))